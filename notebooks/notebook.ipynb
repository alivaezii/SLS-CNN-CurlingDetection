{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7f0316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| split | curling | ohne_curling | total |\n",
      "|------:|-------:|-------------:|------:|\n",
      "| train | 330 | 43194 | 43524 |\n",
      "| val | 73 | 31827 | 31900 |\n",
      "| test | 57 | 715 | 772 |\n",
      "\n",
      "**ROOT:** `G:\\procnn\\dataset_final_RawV5`\n"
     ]
    }
   ],
   "source": [
    "# Minimal dataset summary → Markdown table\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "ROOT = Path(r\"G:/procnn/dataset_final_RawV5\")  # change if needed\n",
    "CLASSES = [\"curling\", \"ohne_curling\"]\n",
    "EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".tif\", \".tiff\", \".gif\"}\n",
    "\n",
    "def count_dir(p: Path) -> int:\n",
    "    n = 0\n",
    "    for r, _, files in os.walk(p):\n",
    "        for fn in files:\n",
    "            if Path(fn).suffix.lower() in EXTS:\n",
    "                n += 1\n",
    "    return n\n",
    "\n",
    "rows = []\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    base = ROOT / split\n",
    "    counts = {c: (count_dir(base / c) if (base / c).exists() else 0) for c in CLASSES}\n",
    "    total = sum(counts.values())\n",
    "    rows.append((split, counts[\"curling\"], counts[\"ohne_curling\"], total))\n",
    "\n",
    "# Print Markdown\n",
    "print(\"| split | curling | ohne_curling | total |\")\n",
    "print(\"|------:|-------:|-------------:|------:|\")\n",
    "for split, c_pos, c_neg, total in rows:\n",
    "    print(f\"| {split} | {c_pos} | {c_neg} | {total} |\")\n",
    "print(f\"\\n**ROOT:** `{ROOT}`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c98da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original counts:\n",
      "  train: {'curling': 330, 'ohne_curling': 43194}\n",
      "  val  : {'curling': 73, 'ohne_curling': 31827}\n",
      "  test : {'curling': 57, 'ohne_curling': 715}\n",
      "\n",
      "Hybrid-4 subsets:\n",
      "  train(hybrid): {'curling': 1980, 'ohne_curling': 3960} | info: {'minority_orig': 330, 'majority_orig': 43194, 'minority_eff': 1980, 'majority_eff': 3960, 'hybrid_total': 5940, 'ratio_eff(ohne:curling)': 2.0}\n",
      "  val(balanced): {'curling': 73, 'ohne_curling': 73} | info: {'per_class': 73, 'total': 146}\n",
      "  test(raw)    : {'curling': 57, 'ohne_curling': 715}\n"
     ]
    }
   ],
   "source": [
    "# ===================== Block B (Hybrid-4): Datasets + Hybrid Train + Balanced Val =====================\n",
    "import os, numpy as np, torch, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Config\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "ROOT = Path(r\"G:/procnn/dataset_final_RawV5\")  # fixed dataset root\n",
    "LABEL_TO_IDX = {\"ohne_curling\": 0, \"curling\": 1}\n",
    "IDX_TO_LABEL = {v: k for k, v in LABEL_TO_IDX.items()}\n",
    "EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".gif\"}\n",
    "\n",
    "def is_image(p: Path) -> bool:\n",
    "    return p.is_file() and p.suffix.lower() in EXTS\n",
    "\n",
    "class ImageFolderFlat(Dataset):\n",
    "    def __init__(self, root: Path, split: str, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        base = root / split\n",
    "        for label, y in LABEL_TO_IDX.items():\n",
    "            d = base / label\n",
    "            if not d.exists():\n",
    "                continue\n",
    "            for r, _, files in os.walk(d):\n",
    "                for fn in files:\n",
    "                    if Path(fn).suffix.lower() in EXTS:\n",
    "                        self.samples.append((Path(r) / fn, y))\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No images under {base} for {list(LABEL_TO_IDX.keys())}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, y = self.samples[i]\n",
    "        im = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        return im, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def orig_counts(ds):\n",
    "    c = Counter([y for _, y in ds.samples])\n",
    "    return {\"curling\": int(c.get(1, 0)), \"ohne_curling\": int(c.get(0, 0))}\n",
    "\n",
    "# Raw datasets\n",
    "ds_train_raw = ImageFolderFlat(ROOT, \"train\")\n",
    "ds_val_raw   = ImageFolderFlat(ROOT, \"val\")\n",
    "ds_test_raw  = ImageFolderFlat(ROOT, \"test\")\n",
    "\n",
    "print(\"Original counts:\")\n",
    "print(\"  train:\", orig_counts(ds_train_raw))\n",
    "print(\"  val  :\", orig_counts(ds_val_raw))\n",
    "print(\"  test :\", orig_counts(ds_test_raw))\n",
    "\n",
    "# Hybrid indices (oversample minority + undersample majority)\n",
    "def make_hybrid_indices(samples, minority_label=1, majority_label=0,\n",
    "                        oversample_factor=6, max_majority_factor=2, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    labels = np.fromiter((y for _, y in samples), dtype=np.int64)\n",
    "    idx_pos = np.where(labels == minority_label)[0]\n",
    "    idx_neg = np.where(labels == majority_label)[0]\n",
    "    if len(idx_pos) == 0 or len(idx_neg) == 0:\n",
    "        raise RuntimeError(\"Both classes must exist in train to build hybrid.\")\n",
    "\n",
    "    n_pos_eff = len(idx_pos) * oversample_factor\n",
    "    n_neg_eff = min(len(idx_neg), int(n_pos_eff * max_majority_factor))\n",
    "\n",
    "    sel_pos = rng.choice(idx_pos, size=n_pos_eff, replace=True)\n",
    "    sel_neg = rng.choice(idx_neg, size=n_neg_eff, replace=False)\n",
    "    sel_idx = np.concatenate([sel_pos, sel_neg])\n",
    "    rng.shuffle(sel_idx)\n",
    "\n",
    "    info = {\n",
    "        \"minority_orig\": int(len(idx_pos)), \"majority_orig\": int(len(idx_neg)),\n",
    "        \"minority_eff\": int(n_pos_eff), \"majority_eff\": int(n_neg_eff),\n",
    "        \"hybrid_total\": int(sel_idx.size),\n",
    "        \"ratio_eff(ohne:curling)\": round(n_neg_eff / max(n_pos_eff, 1), 3)\n",
    "    }\n",
    "    return sel_idx.tolist(), info\n",
    "\n",
    "# Balanced validation (1:1)\n",
    "def make_balanced_val_indices(samples, per_class=800, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    labels = np.fromiter((y for _, y in samples), dtype=np.int64)\n",
    "    idx_pos = np.where(labels == 1)[0]\n",
    "    idx_neg = np.where(labels == 0)[0]\n",
    "    k = int(min(per_class, len(idx_pos), len(idx_neg)))\n",
    "    if k == 0:\n",
    "        raise RuntimeError(\"VAL must contain both classes to build a balanced subset.\")\n",
    "    sel = np.concatenate([\n",
    "        rng.choice(idx_pos, k, replace=False),\n",
    "        rng.choice(idx_neg, k, replace=False)\n",
    "    ])\n",
    "    rng.shuffle(sel)\n",
    "    return sel.tolist(), {\"per_class\": k, \"total\": int(2 * k)}\n",
    "\n",
    "# Build subsets\n",
    "train_idx, train_info = make_hybrid_indices(ds_train_raw.samples, rng=rng)\n",
    "val_idx,   val_info   = make_balanced_val_indices(ds_val_raw.samples, rng=rng)\n",
    "\n",
    "ds_train = Subset(ds_train_raw, train_idx)\n",
    "ds_val   = Subset(ds_val_raw,   val_idx)\n",
    "ds_test  = ds_test_raw\n",
    "\n",
    "def subset_counts(subset, which=\"train\"):\n",
    "    if which == \"train\":\n",
    "        labs = [ds_train_raw.samples[i][1] for i in subset.indices]\n",
    "    elif which == \"val\":\n",
    "        labs = [ds_val_raw.samples[i][1] for i in subset.indices]\n",
    "    else:\n",
    "        labs = [y for _, y in subset.samples]\n",
    "    c = Counter(labs)\n",
    "    return {\"curling\": int(c.get(1, 0)), \"ohne_curling\": int(c.get(0, 0))}\n",
    "\n",
    "print(\"\\nHybrid-4 subsets:\")\n",
    "print(\"  train(hybrid):\", subset_counts(ds_train, \"train\"), \"| info:\", train_info)\n",
    "print(\"  val(balanced):\", subset_counts(ds_val, \"val\"),   \"| info:\", val_info)\n",
    "print(\"  test(raw)    :\", subset_counts(ds_test, \"test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6706fd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\sls-cnn\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\sls-cnn\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying hybrid train to SSD:  52%|█████▏    | 3091/5940 [01:48<01:18, 36.37file/s]"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "\n",
    "import os, shutil, pathlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEST = pathlib.Path(r\"C:/tmp_train_hybrid\")\n",
    "DEST.mkdir(parents=True, exist_ok=True)\n",
    "(DEST / \"curling\").mkdir(exist_ok=True)\n",
    "(DEST / \"ohne_curling\").mkdir(exist_ok=True)\n",
    "\n",
    "# Collect source files from ds_train subset\n",
    "src_paths = []\n",
    "for i in ds_train.indices:  # ds_train = Subset(ds_train_raw, train_idx)\n",
    "    p, y = ds_train_raw.samples[i]\n",
    "    cls = \"curling\" if y == 1 else \"ohne_curling\"\n",
    "    src_paths.append((p, cls))\n",
    "\n",
    "# Copy with progress\n",
    "for p, cls in tqdm(src_paths, desc=\"Copying hybrid train to SSD\", unit=\"file\"):\n",
    "    dst = DEST / cls / p.name\n",
    "    if not dst.exists():\n",
    "        try:\n",
    "            shutil.copy2(p, dst)\n",
    "        except Exception as e:\n",
    "            print(\"Copy error:\", p, \"->\", dst, \"|\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e98eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing stats (SSD): 100%|██████████| 34/34 [00:06<00:00,  4.97batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train mean: [0.4680688679218292, 0.20129556953907013, 0.3361990749835968]\n",
      "Train std : [0.394273579120636, 0.29281333088874817, 0.21700330078601837]\n",
      "Elapsed: 6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'norm_stats.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain std :\u001b[39m\u001b[38;5;124m\"\u001b[39m, std\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnorm_stats.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     59\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m: mean\u001b[38;5;241m.\u001b[39mtolist(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m\"\u001b[39m: std\u001b[38;5;241m.\u001b[39mtolist()}, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\SLS-CNN\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'norm_stats.json'"
     ]
    }
   ],
   "source": [
    "# ===================== Block C (SSD) : Compute Normalization Stats with progress =====================\n",
    "import json, time, torch, pathlib\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "from tqdm import tqdm\n",
    "\n",
    "SSD_ROOT = pathlib.Path(r\"C:/tmp_train_hybrid\")\n",
    "EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".gif\"}\n",
    "\n",
    "class SimpleFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for cls, y in [(\"curling\",1), (\"ohne_curling\",0)]:\n",
    "            d = root/cls\n",
    "            if not d.exists(): \n",
    "                continue\n",
    "            for r,_,files in os.walk(d):\n",
    "                for fn in files:\n",
    "                    if pathlib.Path(fn).suffix.lower() in EXTS:\n",
    "                        self.samples.append((pathlib.Path(r)/fn, y))\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No images under {root}\")\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        p, y = self.samples[i]\n",
    "        im = Image.open(p).convert(\"RGB\")\n",
    "        im = self.transform(im) if self.transform else im\n",
    "        return im, y\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "train_ds_ssd = SimpleFolder(SSD_ROOT, transform=to_tensor)\n",
    "loader = DataLoader(train_ds_ssd, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "n_pixels = 0\n",
    "sum_ = torch.zeros(3)\n",
    "sum_sq = torch.zeros(3)\n",
    "\n",
    "t0 = time.time()\n",
    "for imgs, _ in tqdm(loader, desc=\"Computing stats (SSD)\", unit=\"batch\"):\n",
    "    b, c, h, w = imgs.shape\n",
    "    n_pixels += b * h * w\n",
    "    sum_ += imgs.sum(dim=[0,2,3])\n",
    "    sum_sq += (imgs ** 2).sum(dim=[0,2,3])\n",
    "\n",
    "mean = (sum_ / n_pixels)\n",
    "std = ((sum_sq / n_pixels) - mean**2).sqrt()\n",
    "\n",
    "print(\"Train mean:\", mean.tolist())\n",
    "print(\"Train std :\", std.tolist())\n",
    "print(f\"Elapsed: {time.time() - t0:.1f}s\")\n",
    "\n",
    "with open(\"norm_stats.json\", \"w\") as f:\n",
    "    json.dump({\"mean\": mean.tolist(), \"std\": std.tolist()}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caffb526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready:\n",
      "train=5940 | val=146 | test=772\n",
      "| mean=[0.4680688679218292, 0.20129556953907013, 0.3361990749835968] | std=[0.394273579120636, 0.29281333088874817, 0.21700330078601837]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup pass (train):   0%|          | 0/93 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup pass (train):   2%|▏         | 2/93 [00:00<00:38,  2.38batch/s]\n",
      "Warmup pass (val):   0%|          | 0/3 [00:05<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup OK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================== Block D: Transforms + DataLoaders (CPU-friendly, anti-leakage) =====================\n",
    "import json, pathlib\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "# 1) load normalization stats (from Block C)\n",
    "stats_path = pathlib.Path(\"norm_stats.json\")\n",
    "if stats_path.exists():\n",
    "    stats = json.loads(stats_path.read_text())\n",
    "    MEAN, STD = stats[\"mean\"], stats[\"std\"]\n",
    "else:\n",
    "    MEAN = [0.4680688679, 0.2012955695, 0.33619907498]\n",
    "    STD  = [0.3942735791, 0.2928133309, 0.2170033008]\n",
    "\n",
    "# 2) transforms (train: light hybrid; val/test: deterministic)\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.9, 1.0)),\n",
    "    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "    transforms.RandomErasing(p=0.05, scale=(0.02, 0.08), ratio=(0.3, 3.3), inplace=False),\n",
    "])\n",
    "\n",
    "valtest_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "# 3) apply transforms to Block B datasets via a wrapper\n",
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, base_subset, base_raw, transform=None):\n",
    "        self.base_subset = base_subset\n",
    "        self.base_raw = base_raw\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.base_subset.indices) if hasattr(self.base_subset, \"indices\") else len(self.base_subset)\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.base_subset.indices[i]\n",
    "        path, y = self.base_raw.samples[idx]\n",
    "        from PIL import Image\n",
    "        im = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform: im = self.transform(im)\n",
    "        return im, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, base_raw, transform=None):\n",
    "        self.base_raw = base_raw\n",
    "        self.transform = transform\n",
    "        self.samples = base_raw.samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        path, y = self.samples[i]\n",
    "        from PIL import Image\n",
    "        im = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform: im = self.transform(im)\n",
    "        return im, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# expects ds_train_raw, ds_val_raw, ds_test_raw, ds_train (Subset), ds_val (Subset) from Block B\n",
    "train_ds = TransformedSubset(ds_train, ds_train_raw, transform=train_tf)\n",
    "val_ds   = TransformedSubset(ds_val,   ds_val_raw,   transform=valtest_tf)\n",
    "test_ds  = TransformedDataset(ds_test_raw,           transform=valtest_tf)\n",
    "\n",
    "# 4) DataLoaders (Windows/USB-friendly defaults)\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "PREFETCH = 2\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, pin_memory=False)\n",
    "\n",
    "\n",
    "print(\"DataLoaders ready:\",\n",
    "      f\"train={len(train_ds)} | val={len(val_ds)} | test={len(test_ds)}\",\n",
    "      f\"| mean={MEAN} | std={STD}\", sep=\"\\n\")\n",
    "\n",
    "# 5) quick sanity pass with tqdm (one epoch-like pass without training)\n",
    "from tqdm import tqdm\n",
    "cnt = 0\n",
    "for x, y in tqdm(train_loader, desc=\"Warmup pass (train)\", unit=\"batch\"):\n",
    "    cnt += 1\n",
    "    if cnt >= 3: break  # short warmup\n",
    "for x, y in tqdm(val_loader, desc=\"Warmup pass (val)\", unit=\"batch\"):\n",
    "    break\n",
    "print(\"Warmup OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20 [linear-probe]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL === acc=0.9589 macroF1=0.9588 | F1(curling)=0.9605\n",
      "CM [rows=true, cols=pred]: [[67, 6], [0, 73]]\n",
      "train: loss=0.0036 acc=0.8983 | val: loss=0.0053 acc=0.9589 | val F1(curling)=0.961 macroF1=0.959 | time=125.2s\n",
      "\n",
      "Epoch 2/20 [linear-probe]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL === acc=0.9589 macroF1=0.9588 | F1(curling)=0.9605\n",
      "CM [rows=true, cols=pred]: [[67, 6], [0, 73]]\n",
      "train: loss=0.0013 acc=0.9646 | val: loss=0.0024 acc=0.9589 | val F1(curling)=0.961 macroF1=0.959 | time=145.9s\n",
      "\n",
      "Epoch 3/20 [linear-probe]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL === acc=0.9521 macroF1=0.9519 | F1(curling)=0.9542\n",
      "CM [rows=true, cols=pred]: [[66, 7], [0, 73]]\n",
      "train: loss=0.0011 acc=0.9737 | val: loss=0.0033 acc=0.9521 | val F1(curling)=0.954 macroF1=0.952 | time=279.2s\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL === acc=0.9932 macroF1=0.9932 | F1(curling)=0.9932\n",
      "CM [rows=true, cols=pred]: [[72, 1], [0, 73]]\n",
      "train: loss=0.0015 acc=0.9739 | val: loss=0.0002 acc=0.9932 | val F1(curling)=0.993 macroF1=0.993 | time=344.2s\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL === acc=0.9795 macroF1=0.9794 | F1(curling)=0.9799\n",
      "CM [rows=true, cols=pred]: [[70, 3], [0, 73]]\n",
      "train: loss=0.0002 acc=0.9919 | val: loss=0.0034 acc=0.9795 | val F1(curling)=0.980 macroF1=0.979 | time=224.1s\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL === acc=0.8836 macroF1=0.8820 | F1(curling)=0.8682\n",
      "CM [rows=true, cols=pred]: [[73, 0], [17, 56]]\n",
      "train: loss=0.0003 acc=0.9936 | val: loss=0.0316 acc=0.8836 | val F1(curling)=0.868 macroF1=0.882 | time=329.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL === acc=0.9658 macroF1=0.9657 | F1(curling)=0.9645\n",
      "CM [rows=true, cols=pred]: [[73, 0], [5, 68]]\n",
      "train: loss=0.0002 acc=0.9931 | val: loss=0.0083 acc=0.9658 | val F1(curling)=0.965 macroF1=0.966 | time=316.6s\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt — saving checkpoint.\n",
      "Best weights at: mobilenetv3_focal_pretrained_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VAL_BEST === acc=0.9932 macroF1=0.9932 | F1(curling)=0.9932\n",
      "CM [rows=true, cols=pred]: [[72, 1], [0, 73]]\n",
      "\n",
      "Best threshold on VAL for curling=1: th=0.666 | F1=1.000 (P=1.000, R=1.000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "metrics:   8%|▊         | 1/13 [00:03<00:42,  3.54s/batch]"
     ]
    }
   ],
   "source": [
    "# ===================== Block E: MobileNetV3-Small (pretrained) + Focal Loss + Warmup + Resume-Safe =====================\n",
    "import os, math, time, csv, json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix,\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "\n",
    "# -------- Config --------\n",
    "DEVICE = torch.device(\"cpu\")  # CPU-friendly\n",
    "NUM_CLASSES = 2\n",
    "EPOCHS = 20\n",
    "WARMUP_EPOCHS = 3\n",
    "PATIENCE = 6\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "FOCAL_GAMMA = 2.0\n",
    "ALPHA = torch.tensor([0.25, 0.75], dtype=torch.float32)  # [neg, pos]\n",
    "\n",
    "CSV_LOG = \"train_log.csv\"\n",
    "BEST_PATH = \"mobilenetv3_focal_pretrained_best.pth\"\n",
    "CKPT_PATH = \"train_ckpt.pth\"\n",
    "TEST_REPORT = \"test_report.json\"\n",
    "\n",
    "# -------- Focal Loss --------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, targets):\n",
    "        ce = nn.functional.cross_entropy(logits, targets, reduction=\"none\", weight=self.alpha)\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# -------- Model (pretrained) --------\n",
    "def build_model(num_classes=2):\n",
    "    weights = MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "    m = mobilenet_v3_small(weights=weights)\n",
    "    in_feats = m.classifier[3].in_features\n",
    "    m.classifier[3] = nn.Linear(in_feats, num_classes)\n",
    "    return m\n",
    "\n",
    "model = build_model(NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "alpha_device = ALPHA.to(DEVICE)\n",
    "criterion = FocalLoss(alpha=alpha_device, gamma=FOCAL_GAMMA)\n",
    "\n",
    "# freeze backbone for warmup\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda t: t.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def run_epoch(loader, train_mode=True):\n",
    "    model.train(train_mode)\n",
    "    total_loss, total_correct, total_samp = 0.0, 0, 0\n",
    "    for x, y in tqdm(loader, desc=\"train\" if train_mode else \"val\", unit=\"batch\", leave=False):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * y.size(0)\n",
    "        total_correct += (logits.argmax(1) == y).sum().item()\n",
    "        total_samp += y.size(0)\n",
    "    return total_loss / max(1, total_samp), total_correct / max(1, total_samp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_preds(loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    for x, y in tqdm(loader, desc=\"metrics\", unit=\"batch\", leave=False):\n",
    "        x = x.to(DEVICE)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        y_true.extend(y.numpy().tolist())\n",
    "        y_pred.extend(probs.argmax(1).tolist())\n",
    "        y_prob.extend(probs[:, 1].tolist())  # prob of class-1\n",
    "    return torch.tensor(y_true).numpy(), torch.tensor(y_pred).numpy(), torch.tensor(y_prob).numpy()\n",
    "\n",
    "def eval_split(loader, name=\"val\"):\n",
    "    y_true, y_pred, y_prob = collect_preds(loader)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p_cls, r_cls, f1_cls, supp = precision_recall_fscore_support(y_true, y_pred, labels=[0,1], average=None, zero_division=0)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    print(f\"\\n=== {name.upper()} === acc={acc:.4f} macroF1={f1_macro:.4f} | F1(curling)={f1_cls[1]:.4f}\")\n",
    "    print(\"CM [rows=true, cols=pred]:\", cm.tolist())\n",
    "    return {\"accuracy\": float(acc), \"macro_f1\": float(f1_macro), \"f1_curling\": float(f1_cls[1]),\n",
    "            \"cm\": cm.tolist(), \"y_true\": y_true.tolist(), \"y_prob\": y_prob.tolist()}\n",
    "\n",
    "# -------- Checkpoint helpers --------\n",
    "def save_ckpt(epoch, model, optimizer, scheduler, best_state, best_val, no_improve):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict() if scheduler else None,\n",
    "        \"best_state\": best_state,\n",
    "        \"best_val\": best_val,\n",
    "        \"no_improve\": no_improve\n",
    "    }, CKPT_PATH)\n",
    "\n",
    "def load_ckpt(model, optimizer=None, scheduler=None):\n",
    "    if not os.path.exists(CKPT_PATH): return None\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    if optimizer and ckpt.get(\"optimizer\"): optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    if scheduler and ckpt.get(\"scheduler\"): scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    return ckpt\n",
    "\n",
    "# -------- Training loop --------\n",
    "start_epoch = 1\n",
    "loaded = load_ckpt(model, optimizer, scheduler)\n",
    "if loaded:\n",
    "    start_epoch = loaded[\"epoch\"] + 1\n",
    "    best_state, best_val, no_improve = loaded[\"best_state\"], loaded[\"best_val\"], loaded[\"no_improve\"]\n",
    "    print(f\"Resumed from epoch {loaded['epoch']} | best_val={best_val:.6f}\")\n",
    "else:\n",
    "    best_state, best_val, no_improve = None, math.inf, 0\n",
    "\n",
    "if not os.path.exists(CSV_LOG):\n",
    "    with open(CSV_LOG, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"epoch\",\"phase\",\"loss\",\"acc\",\"val_macro_f1\",\"val_f1_curling\"])\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, EPOCHS + 1):\n",
    "        if epoch == WARMUP_EPOCHS + 1:\n",
    "            for p in model.features.parameters(): p.requires_grad = True\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}/{EPOCHS}\" + (\" [linear-probe]\" if epoch <= WARMUP_EPOCHS else \"\"))\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = run_epoch(train_loader, True)\n",
    "        val_loss, val_acc = run_epoch(val_loader, False)\n",
    "        val_metrics = eval_split(val_loader, \"val\")\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        with open(CSV_LOG, \"a\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([epoch,\"train\",f\"{tr_loss:.6f}\",f\"{tr_acc:.6f}\",\"\",\"\"])\n",
    "            w.writerow([epoch,\"val\",f\"{val_loss:.6f}\",f\"{val_acc:.6f}\",\n",
    "                        f\"{val_metrics['macro_f1']:.6f}\",f\"{val_metrics['f1_curling']:.6f}\"])\n",
    "\n",
    "        print(f\"train: loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
    "              f\"val: loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
    "              f\"val F1(curling)={val_metrics['f1_curling']:.3f} macroF1={val_metrics['macro_f1']:.3f} | \"\n",
    "              f\"time={time.time()-t0:.1f}s\")\n",
    "\n",
    "        if val_loss < best_val - 1e-4:\n",
    "            best_val = val_loss\n",
    "            best_state = {k: v.cpu().clone() for k,v in model.state_dict().items()}\n",
    "            torch.save(model.state_dict(), BEST_PATH)\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= PATIENCE:\n",
    "                print(\"Early stopping.\"); break\n",
    "\n",
    "        save_ckpt(epoch, model, optimizer, scheduler, best_state, best_val, no_improve)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nKeyboardInterrupt — saving checkpoint.\")\n",
    "    save_ckpt(epoch, model, optimizer, scheduler, best_state, best_val, no_improve)\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "print(f\"Best weights at: {BEST_PATH}\")\n",
    "\n",
    "# -------- Threshold tuning on VAL --------\n",
    "@torch.no_grad()\n",
    "def best_threshold_on_val(val_metrics):\n",
    "    y_true = torch.tensor(val_metrics[\"y_true\"]).numpy()\n",
    "    y_prob = torch.tensor(val_metrics[\"y_prob\"]).numpy()\n",
    "    ps, rs, ths = precision_recall_curve(y_true, y_prob)\n",
    "    f1s = 2 * (ps*rs) / (ps+rs+1e-12)\n",
    "    idx = f1s.argmax()\n",
    "    best_th = float(ths[max(0, idx-1)]) if idx < len(ths) else 0.5\n",
    "    return float(best_th), float(f1s[idx]), float(ps[idx]), float(rs[idx])\n",
    "\n",
    "val_metrics_final = eval_split(val_loader, \"val_best\")\n",
    "best_th, best_f1, best_p, best_r = best_threshold_on_val(val_metrics_final)\n",
    "print(f\"\\nBest threshold on VAL for curling=1: th={best_th:.3f} | F1={best_f1:.3f} (P={best_p:.3f}, R={best_r:.3f})\")\n",
    "\n",
    "# -------- Final TEST evaluation --------\n",
    "@torch.no_grad()\n",
    "def eval_test_with_threshold(th):\n",
    "    y_true, y_pred_argmax, y_prob = collect_preds(test_loader)\n",
    "    y_pred_th = (y_prob >= th).astype(int)\n",
    "\n",
    "    def metrics(y_true, y_pred, name):\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[0,1], average=None, zero_division=0)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "        return {\"acc\": float(acc),\n",
    "                \"precision\": {\"ohne\": float(p[0]), \"curling\": float(p[1])},\n",
    "                \"recall\":    {\"ohne\": float(r[0]), \"curling\": float(r[1])},\n",
    "                \"f1\":        {\"ohne\": float(f1[0]), \"curling\": float(f1[1])},\n",
    "                \"cm\": cm.tolist()}\n",
    "\n",
    "    return {\"argmax\": metrics(y_true, y_pred_argmax, \"argmax\"),\n",
    "            \"thresholded\": {\"threshold\": float(th), **metrics(y_true, y_pred_th, \"th\")} }\n",
    "\n",
    "test_report = eval_test_with_threshold(best_th)\n",
    "print(\"\\n=== TEST (argmax) ===\", json.dumps(test_report[\"argmax\"], indent=2))\n",
    "print(\"\\n=== TEST (thresholded) ===\", json.dumps(test_report[\"thresholded\"], indent=2))\n",
    "\n",
    "with open(TEST_REPORT, \"w\") as f:\n",
    "    json.dump(test_report, f, indent=2)\n",
    "print(f\"Saved: {TEST_REPORT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "276dec07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL counts: {'curling': 73, 'ohne_curling': 31827} | TEST counts: {'curling': 57, 'ohne_curling': 715}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliva\\AppData\\Local\\Temp\\ipykernel_16508\\1532535324.py:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(str(best_path), map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best weights: mobilenetv3_focal_pretrained_best.pth | size=6070.5 KB\n",
      "[VAL] best_threshold = 0.513 → saved to G:\\procnn\\val_threshold.json\n",
      "[TEST] saved to G:\\procnn\\test_report.json\n"
     ]
    }
   ],
   "source": [
    "# ===================== Unified Block: Prep → Threshold on VAL → Final TEST =====================\n",
    "import os, glob, json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, precision_recall_fscore_support,\n",
    "    accuracy_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "PROJ_ROOT = Path(r\"G:\\procnn\")                       # project root\n",
    "DATA_ROOT = PROJ_ROOT / \"dataset_final_RawV5\"        # must contain train/val/test\n",
    "BEST_NAME = \"mobilenetv3_focal_pretrained_best.pth\"  # best weights saved from training\n",
    "NORM_PATH = PROJ_ROOT / \"norm_stats.json\"            # mean/std computed on train(hybrid)\n",
    "VAL_THRESHOLD_JSON = PROJ_ROOT / \"val_threshold.json\"\n",
    "TEST_REPORT_JSON = PROJ_ROOT / \"test_report.json\"\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "LABEL_TO_IDX = {\"ohne_curling\": 0, \"curling\": 1}\n",
    "IDX_TO_LABEL = {v: k for k, v in LABEL_TO_IDX.items()}\n",
    "\n",
    "IMG_EXTS = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".gif\",\n",
    "            \".JPG\",\".JPEG\",\".PNG\",\".BMP\",\".WEBP\",\".TIF\",\".TIFF\",\".GIF\"}\n",
    "\n",
    "# ---------------- Utils ----------------\n",
    "def is_image(p: Path) -> bool:\n",
    "    return p.is_file() and p.suffix in IMG_EXTS\n",
    "\n",
    "class ImageFolderFlat(Dataset):\n",
    "    def __init__(self, root: Path, split: str, transform=None):\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        base = root / split\n",
    "        if not base.exists():\n",
    "            raise FileNotFoundError(f\"Split folder not found: {base}\")\n",
    "        for label, y in LABEL_TO_IDX.items():\n",
    "            d = base / label\n",
    "            if not d.exists(): \n",
    "                continue\n",
    "            for q in d.rglob(\"*\"):\n",
    "                if is_image(q):\n",
    "                    self.samples.append((q, y))\n",
    "        if not self.samples:\n",
    "            raise RuntimeError(f\"No images under {base}\")\n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, i):\n",
    "        path, y = self.samples[i]\n",
    "        im = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            im = self.transform(im)\n",
    "        return im, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def find_best_pth() -> Path:\n",
    "    p = PROJ_ROOT / BEST_NAME\n",
    "    if p.is_file():\n",
    "        return p\n",
    "    hits = [Path(f) for f in glob.glob(str(PROJ_ROOT / \"**\" / BEST_NAME), recursive=True)]\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "    hits = [Path(f) for f in glob.glob(str(PROJ_ROOT / \"**\" / \"*.pth\"), recursive=True)]\n",
    "    hits = [f for f in hits if \"best\" in f.name.lower()]\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "    raise FileNotFoundError(f\"Best weights not found under {PROJ_ROOT}\")\n",
    "\n",
    "def pack_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[0,1], average=None, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1]).tolist()\n",
    "    return {\n",
    "        \"acc\": float(acc),\n",
    "        \"precision\": {\"ohne_curling\": float(p[0]), \"curling\": float(p[1])},\n",
    "        \"recall\":    {\"ohne_curling\": float(r[0]), \"curling\": float(r[1])},\n",
    "        \"f1\":        {\"ohne_curling\": float(f1[0]), \"curling\": float(f1[1])},\n",
    "        \"cm\": cm\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_probs(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    y_true, y_prob, y_pred_arg = [], [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        y_prob.extend(probs[:, 1])               # prob for class=1\n",
    "        y_pred_arg.extend(np.argmax(probs, 1))   # argmax\n",
    "        y_true.extend(y.numpy())\n",
    "    return np.array(y_true), np.array(y_prob), np.array(y_pred_arg)\n",
    "\n",
    "# ---------------- Prep DataLoaders ----------------\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"DATA_ROOT not found: {DATA_ROOT}\")\n",
    "\n",
    "if not NORM_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Normalization stats not found: {NORM_PATH} (expected from training)\")\n",
    "\n",
    "stats = json.load(open(NORM_PATH, \"r\"))\n",
    "MEAN, STD = stats[\"mean\"], stats[\"std\"]\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\n",
    "])\n",
    "\n",
    "ds_val  = ImageFolderFlat(DATA_ROOT, \"val\",  transform=eval_tf)\n",
    "ds_test = ImageFolderFlat(DATA_ROOT, \"test\", transform=eval_tf)\n",
    "\n",
    "val_loader  = DataLoader(ds_val,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "\n",
    "def counts(ds: ImageFolderFlat):\n",
    "    c = Counter([y for _, y in ds.samples])\n",
    "    return {\"curling\": int(c.get(1,0)), \"ohne_curling\": int(c.get(0,0))}\n",
    "print(\"VAL counts:\", counts(ds_val), \"| TEST counts:\", counts(ds_test))\n",
    "\n",
    "# ---------------- Build & Load Model ----------------\n",
    "def build_model(num_classes=2):\n",
    "    w = MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "    m = mobilenet_v3_small(weights=w)\n",
    "    in_feats = m.classifier[3].in_features\n",
    "    m.classifier[3] = nn.Linear(in_feats, num_classes)\n",
    "    return m\n",
    "\n",
    "model = build_model(2).to(DEVICE)\n",
    "best_path = find_best_pth()\n",
    "state = torch.load(str(best_path), map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "print(f\"Loaded best weights: {best_path.name} | size={best_path.stat().st_size/1024:.1f} KB\")\n",
    "\n",
    "# ---------------- 1) Threshold tuning on VAL ----------------\n",
    "yv_true, yv_prob, yv_pred_arg = collect_probs(model, val_loader, DEVICE)\n",
    "prec, rec, ths = precision_recall_curve(yv_true, yv_prob)  # positive class = 1\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "if len(ths) == 0:\n",
    "    best_th = 0.5\n",
    "else:\n",
    "    best_idx = int(np.argmax(f1s))\n",
    "    best_th = float(ths[max(0, min(best_idx, len(ths)-1))])\n",
    "\n",
    "yv_pred_th = (yv_prob >= best_th).astype(int)\n",
    "\n",
    "val_summary = {\n",
    "    \"best_threshold\": best_th,\n",
    "    \"val_argmax\": pack_metrics(yv_true, yv_pred_arg),\n",
    "    \"val_thresholded\": {\"threshold\": best_th, **pack_metrics(yv_true, yv_pred_th)}\n",
    "}\n",
    "with open(VAL_THRESHOLD_JSON, \"w\") as f:\n",
    "    json.dump(val_summary, f, indent=2)\n",
    "print(f\"[VAL] best_threshold = {best_th:.3f} → saved to {VAL_THRESHOLD_JSON}\")\n",
    "\n",
    "# ---------------- 2) Final TEST evaluation ----------------\n",
    "yt_true, yt_prob, yt_pred_arg = collect_probs(model, test_loader, DEVICE)\n",
    "yt_pred_th = (yt_prob >= best_th).astype(int)\n",
    "\n",
    "test_report = {\n",
    "    \"argmax\": pack_metrics(yt_true, yt_pred_arg),\n",
    "    \"thresholded\": {\"threshold\": best_th, **pack_metrics(yt_true, yt_pred_th)}\n",
    "}\n",
    "with open(TEST_REPORT_JSON, \"w\") as f:\n",
    "    json.dump(test_report, f, indent=2)\n",
    "print(f\"[TEST] saved to {TEST_REPORT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bdfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] Best Threshold = 0.5134\n",
      "\n",
      "=== VAL—ARGMAX ===\n",
      "\n",
      "=== VAL—THRESHOLDED ===\n",
      "Threshold used: 0.5134\n",
      "\n",
      "=== TEST ===\n",
      "\n",
      "-- ARGMAX --\n",
      "Accuracy: 1.0000\n",
      "Precision: {'ohne_curling': 1.0, 'curling': 1.0}\n",
      "Recall   : {'ohne_curling': 1.0, 'curling': 1.0}\n",
      "F1       : {'ohne_curling': 1.0, 'curling': 1.0}\n",
      "MacroAvg : {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "Support  : {'ohne_curling': 715, 'curling': 57}\n",
      "Specificity(ohne): 0.9999999999999986\n",
      "Confusion Matrix [[TN,FP],[FN,TP]]: [[715, 0], [0, 57]]\n",
      "\n",
      "-- THRESHOLDED (thr=0.5134) --\n",
      "Accuracy: 1.0000\n",
      "Precision: {'ohne_curling': 1.0, 'curling': 1.0}\n",
      "Recall   : {'ohne_curling': 1.0, 'curling': 1.0}\n",
      "F1       : {'ohne_curling': 1.0, 'curling': 1.0}\n",
      "MacroAvg : {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "Support  : {'ohne_curling': 715, 'curling': 57}\n",
      "Specificity(ohne): 0.9999999999999986\n",
      "Confusion Matrix [[TN,FP],[FN,TP]]: [[715, 0], [0, 57]]\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJ_ROOT = Path(r\"G:\\procnn\")\n",
    "val_json = PROJ_ROOT / \"val_threshold.json\"\n",
    "test_json = PROJ_ROOT / \"test_report.json\"\n",
    "\n",
    "def pretty_print_report(report, title=\"REPORT\"):\n",
    "    def calc_macro_avg(d):\n",
    "  \n",
    "        vals = list(d.values())\n",
    "        return float(np.mean(vals)) if vals else float(\"nan\")\n",
    "\n",
    "    def metrics_from_pack(pack):\n",
    "        acc = pack[\"acc\"]\n",
    "        P = pack[\"precision\"]\n",
    "        R = pack[\"recall\"]\n",
    "        F = pack[\"f1\"]\n",
    "        cm = np.array(pack[\"cm\"])\n",
    "        \n",
    "        TN, FP, FN, TP = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "        spec_neg = TN / (TN + FP + 1e-12)  \n",
    "        spec_pos = TP / (TP + FN + 1e-12)  \n",
    "        sup0 = TN + FP  \n",
    "        sup1 = FN + TP  \n",
    "        macroP = calc_macro_avg(P); macroR = calc_macro_avg(R); macroF = calc_macro_avg(F)\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"precision\": P, \"recall\": R, \"f1\": F,\n",
    "            \"macro_avg\": {\"precision\": macroP, \"recall\": macroR, \"f1\": macroF},\n",
    "            \"specificity\": {\"ohne_curling\": float(spec_neg), \"curling\": None}, \n",
    "            \"support\": {\"ohne_curling\": int(sup0), \"curling\": int(sup1)},\n",
    "            \"cm\": cm.astype(int).tolist()\n",
    "        }\n",
    "\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    if \"threshold\" in report:\n",
    "        print(f\"Threshold used: {report['threshold']:.4f}\")\n",
    "\n",
    "    \n",
    "    for key in [\"argmax\", \"thresholded\"]:\n",
    "        if key in report:\n",
    "            block = report[key]\n",
    "            th = block.get(\"threshold\", None)\n",
    "            m = metrics_from_pack(block)\n",
    "            tag = f\"{key.upper()} (thr={th:.4f})\" if th is not None else key.upper()\n",
    "            print(f\"\\n-- {tag} --\")\n",
    "            print(f\"Accuracy: {m['accuracy']:.4f}\")\n",
    "            print(\"Precision:\", m[\"precision\"])\n",
    "            print(\"Recall   :\", m[\"recall\"])\n",
    "            print(\"F1       :\", m[\"f1\"])\n",
    "            print(\"MacroAvg :\", m[\"macro_avg\"])\n",
    "            print(\"Support  :\", m[\"support\"])\n",
    "            print(\"Specificity(ohne):\", m[\"specificity\"][\"ohne_curling\"])\n",
    "            print(\"Confusion Matrix [[TN,FP],[FN,TP]]:\", m[\"cm\"])\n",
    "\n",
    "# VAL\n",
    "with open(val_json, \"r\") as f:\n",
    "    val = json.load(f)\n",
    "print(f\"[VAL] Best Threshold = {val['best_threshold']:.4f}\")\n",
    "pretty_print_report(val[\"val_argmax\"], title=\"VAL—ARGMAX\")\n",
    "pretty_print_report(val[\"val_thresholded\"], title=\"VAL—THRESHOLDED\")\n",
    "\n",
    "# TEST\n",
    "with open(test_json, \"r\") as f:\n",
    "    test = json.load(f)\n",
    "pretty_print_report(test, title=\"TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a2ccb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def write_csv_from_pack(pack, csv_path):\n",
    "    cm = pack[\"cm\"]\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"metric\",\"ohne_curling\",\"curling\",\"macro\"])\n",
    "        w.writerow([\"precision\", pack[\"precision\"][\"ohne_curling\"], pack[\"precision\"][\"curling\"],\n",
    "                    (pack[\"precision\"][\"ohne_curling\"]+pack[\"precision\"][\"curling\"])/2])\n",
    "        w.writerow([\"recall\",    pack[\"recall\"][\"ohne_curling\"],    pack[\"recall\"][\"curling\"],\n",
    "                    (pack[\"recall\"][\"ohne_curling\"]+pack[\"recall\"][\"curling\"])/2])\n",
    "        w.writerow([\"f1\",        pack[\"f1\"][\"ohne_curling\"],        pack[\"f1\"][\"curling\"],\n",
    "                    (pack[\"f1\"][\"ohne_curling\"]+pack[\"f1\"][\"curling\"])/2])\n",
    "        w.writerow([])\n",
    "        w.writerow([\"accuracy\", pack[\"acc\"], \"\", \"\"])\n",
    "        w.writerow([])\n",
    "        w.writerow([\"confusion_matrix_format\",\"[[TN,FP],[FN,TP]]\",\"\",\"\"])\n",
    "        w.writerow([\"row1\", cm[0][0], cm[0][1], \"\"])\n",
    "        w.writerow([\"row2\", cm[1][0], cm[1][1], \"\"])\n",
    "\n",
    "\n",
    "with open(r\"G:\\procnn\\test_report.json\",\"r\") as f:\n",
    "    test = json.load(f)\n",
    "pack = test[\"thresholded\"]\n",
    "write_csv_from_pack(pack, r\"G:\\procnn\\test_report_thresholded.csv\")\n",
    "print(\"CSV saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a75936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
